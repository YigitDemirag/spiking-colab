{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTaXK4DVnwHI"
      },
      "source": [
        "### Training Recurrent Spiking Neural Networks with JAX on TPU or multi-GPU Environments\n",
        "\n",
        "\n",
        "**Author**: [Yigit Demirag](https://github.com/YigitDemirag/spikingTPU), ETH Zurich and University of Zurich, Switzerland\n",
        " \n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "6y9J9i7aLvSG"
      },
      "outputs": [],
      "source": [
        "#@title 1. Import libraries\n",
        "\n",
        "import os\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "    import jax.tools.colab_tpu\n",
        "    jax.tools.colab_tpu.setup_tpu()\n",
        "\n",
        "!pip install einops flax --quiet\n",
        "import urllib.request\n",
        "import gzip, shutil\n",
        "import hashlib\n",
        "import h5py\n",
        "from six.moves.urllib.error import HTTPError \n",
        "from six.moves.urllib.error import URLError\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "import time\n",
        "from functools import partial\n",
        "from einops import repeat, rearrange\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.random as random\n",
        "from jax import vmap, pmap, jit, value_and_grad, local_device_count\n",
        "from jax.lax import scan\n",
        "from jax.nn import log_softmax\n",
        "from jax.example_libraries import optimizers\n",
        "from jax.tree_util import tree_map\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np \n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from flax.jax_utils import prefetch_to_device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "hfXh1F9hIlBc"
      },
      "outputs": [],
      "source": [
        "#@title 2. TFDS Data Pipeline for Spiking Heidelberg Digits (SHD)\n",
        "\"\"\"\n",
        "Taken from \n",
        "    - https://zenkelab.org/resources/spiking-heidelberg-datasets-shd/\n",
        "    - https://github.com/google-research/vision_transformer/blob/master/vit_jax/input_pipeline.py\n",
        "\"\"\"\n",
        "def get_audio_dataset(cache_dir, cache_subdir):\n",
        "    base_url = \"https://zenkelab.org/datasets\"\n",
        "    response = urllib.request.urlopen(\"%s/md5sums.txt\"%base_url)\n",
        "    data = response.read() \n",
        "    lines = data.decode('utf-8').split(\"\\n\")\n",
        "    file_hashes = { line.split()[1]:line.split()[0] \\\n",
        "                    for line in lines if len(line.split())==2 }\n",
        "    files = [ \"shd_train.h5.gz\", \"shd_test.h5.gz\"]\n",
        "        \n",
        "    for fn in files:\n",
        "        origin = \"%s/%s\"%(base_url,fn)\n",
        "        hdf5_file_path = get_and_gunzip(origin, \n",
        "                                        fn, \n",
        "                                        md5hash=file_hashes[fn],\n",
        "                                        cache_dir=cache_dir,\n",
        "                                        cache_subdir=cache_subdir)\n",
        "        print(\"Available at: %s\"%hdf5_file_path)\n",
        "\n",
        "def get_and_gunzip(origin, filename, md5hash=None, cache_dir=None, \n",
        "                   cache_subdir=None):\n",
        "    gz_file_path = get_file(filename, origin, md5_hash=md5hash,\n",
        "                            cache_dir=cache_dir, cache_subdir=cache_subdir)\n",
        "    hdf5_file_path = gz_file_path[:-3]\n",
        "    if not os.path.isfile(hdf5_file_path) or os.path.getctime(gz_file_path) > os.path.getctime(hdf5_file_path):\n",
        "        print(\"Decompressing %s\"%gz_file_path)\n",
        "        with gzip.open(gz_file_path, 'r') as f_in, open(hdf5_file_path, 'wb') as f_out:\n",
        "            shutil.copyfileobj(f_in, f_out)\n",
        "    return hdf5_file_path\n",
        "\n",
        "def validate_file(fpath, file_hash, algorithm='auto', chunk_size=65535):\n",
        "    if (algorithm == 'sha256') or (algorithm == 'auto' and len(file_hash) == 64):\n",
        "        hasher = 'sha256'\n",
        "    else:\n",
        "        hasher = 'md5'\n",
        "    if str(_hash_file(fpath, hasher, chunk_size)) == str(file_hash):\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "def _hash_file(fpath, algorithm='sha256', chunk_size=65535):\n",
        "    if (algorithm == 'sha256') or (algorithm == 'auto' and len(hash) == 64):\n",
        "        hasher = hashlib.sha256()\n",
        "    else:\n",
        "        hasher = hashlib.md5()\n",
        "\n",
        "    with open(fpath, 'rb') as fpath_file:\n",
        "        for chunk in iter(lambda: fpath_file.read(chunk_size), b''):\n",
        "            hasher.update(chunk)\n",
        "\n",
        "    return hasher.hexdigest()\n",
        "\n",
        "def get_file(fname,\n",
        "             origin,\n",
        "             md5_hash=None,\n",
        "             file_hash=None,\n",
        "             cache_subdir='datasets',\n",
        "             hash_algorithm='auto',\n",
        "             extract=False,\n",
        "             archive_format='auto',\n",
        "             cache_dir=None):\n",
        "    if cache_dir is None:\n",
        "        cache_dir = os.path.join(os.path.expanduser('~'), '.data-cache')\n",
        "    if md5_hash is not None and file_hash is None:\n",
        "        file_hash = md5_hash\n",
        "        hash_algorithm = 'md5'\n",
        "    datadir_base = os.path.expanduser(cache_dir)\n",
        "    if not os.access(datadir_base, os.W_OK):\n",
        "        datadir_base = os.path.join('/tmp', '.data-cache')\n",
        "    datadir = os.path.join(datadir_base, cache_subdir)\n",
        "\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "    os.makedirs(datadir, exist_ok=True)\n",
        "\n",
        "    fpath = os.path.join(datadir, fname)\n",
        "\n",
        "    download = False\n",
        "    if os.path.exists(fpath):\n",
        "        if file_hash is not None:\n",
        "            if not validate_file(fpath, file_hash, algorithm=hash_algorithm):\n",
        "                print('A local file was found, but it seems to be '\n",
        "                      'incomplete or outdated because the ' + hash_algorithm +\n",
        "                      ' file hash does not match the original value of ' + file_hash +\n",
        "                      ' so we will re-download the data.')\n",
        "                download = True\n",
        "    else:\n",
        "        download = True\n",
        "\n",
        "    if download:\n",
        "        print('Downloading data from', origin)\n",
        "\n",
        "        error_msg = 'URL fetch failure on {}: {} -- {}'\n",
        "        try:\n",
        "            try:\n",
        "                urlretrieve(origin, fpath)\n",
        "            except HTTPError as e:\n",
        "                raise Exception(error_msg.format(origin, e.code, e.msg))\n",
        "            except URLError as e:\n",
        "                raise Exception(error_msg.format(origin, e.errno, e.reason))\n",
        "        except (Exception, KeyboardInterrupt) as e:\n",
        "            if os.path.exists(fpath):\n",
        "                os.remove(fpath)\n",
        "\n",
        "    return fpath\n",
        "\n",
        "def get_h5py_files():\n",
        "    cache_dir = os.path.expanduser(\"/content/\")\n",
        "    cache_subdir = \"audiospikes\"\n",
        "    get_audio_dataset(cache_dir, cache_subdir)\n",
        "    train_shd_file = h5py.File(os.path.join(cache_dir, cache_subdir, 'shd_train.h5'), 'r') #r\n",
        "    test_shd_file  = h5py.File(os.path.join(cache_dir, cache_subdir, 'shd_test.h5'), 'r')\n",
        "    return train_shd_file, test_shd_file\n",
        "\n",
        "def preprocess_h5py_files(h5py_file):\n",
        "    nb_steps = 100\n",
        "    nb_units = 700\n",
        "    max_time = 1.4 \n",
        "    num_samples = h5py_file['spikes']['times'].shape[0]\n",
        "\n",
        "    firing_times = h5py_file['spikes']['times']\n",
        "    units_fired  = h5py_file['spikes']['units']    \n",
        "    labels       = h5py_file['labels']\n",
        "\n",
        "    time_bins = np.linspace(0, max_time, num=nb_steps)\n",
        "    input  = np.zeros((num_samples, nb_steps, nb_units), dtype=np.uint8)\n",
        "    output = np.array(labels, dtype=np.uint8)\n",
        "\n",
        "    for idx in range(num_samples):\n",
        "        times = np.digitize(firing_times[idx], time_bins)\n",
        "        units = units_fired[idx] \n",
        "        input[idx, times, units] = 1\n",
        "\n",
        "    return input, output\n",
        "\n",
        "num_devices = jax.local_device_count()\n",
        "\n",
        "def shard(data):\n",
        "    data['spikes'] = rearrange(data['spikes'], '(d b) t u -> d b t u', d=num_devices)\n",
        "    data['labels'] = rearrange(data['labels'], '(d b) -> d b', d=num_devices)\n",
        "    return data\n",
        "\n",
        "def create_tf_dataset(input, output, batch_size, training):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices({'spikes':input, \n",
        "                                                  'labels':output})\n",
        "    if training:\n",
        "        dataset = dataset.shuffle(input.shape[0])\n",
        "\n",
        "    dataset = dataset.cache()\n",
        "    if training:\n",
        "        dataset = dataset.batch(batch_size * num_devices, drop_remainder=True)\n",
        "    else:\n",
        "        dataset = dataset.batch(batch_size * num_devices, drop_remainder=False)\n",
        "\n",
        "    dataset = dataset.map(shard, tf.data.AUTOTUNE)\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "    return tfds.as_numpy(dataset)\n",
        "\n",
        "def prefetch(dataset, n_prefetch):\n",
        "    ds_iter = iter(dataset)\n",
        "    ds_iter = map(lambda x: jax.tree_map(lambda t: np.asarray(memoryview(t)), x),\n",
        "                  ds_iter)\n",
        "    if n_prefetch:\n",
        "        ds_iter = prefetch_to_device(ds_iter, n_prefetch)\n",
        "    return ds_iter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "id": "_UFupQ30I_u5"
      },
      "outputs": [],
      "source": [
        "#@title 3. Spiking Neural Network Models (with straight-through estimators)\n",
        "\n",
        "@jax.custom_jvp\n",
        "def gr_than(x, thr):\n",
        "    return (x > thr).astype(jnp.float32)\n",
        " \n",
        "@gr_than.defjvp\n",
        "def gr_jvp(primals, tangents):\n",
        "    x, thr = primals\n",
        "    x_dot, y_dot = tangents\n",
        "    primal_out = gr_than(x, thr)\n",
        "    tangent_out = x_dot * 1 / (jnp.absolute(x-thr)+1)**2\n",
        "    return primal_out, tangent_out\n",
        "\n",
        "def lif_forward(state, x):\n",
        "    ''' Leaky Integrate and Fire (LIF) neuron model\n",
        "    '''\n",
        "    inp_weight, rec_weight, out_weight = state[0]     # Static weights\n",
        "    thr_rec, thr_out, alpha, kappa = state[1]         # Static neuron states\n",
        "    v, z, vo, zo = state[2]                           # Dynamic neuron states\n",
        "\n",
        "    v  = alpha * v + jnp.matmul(x, inp_weight) + jnp.matmul(z, rec_weight) - z * thr_rec\n",
        "    z = gr_than(v, thr_rec)\n",
        "    vo = kappa * vo + jnp.matmul(z, out_weight) - zo * thr_out\n",
        "    zo = gr_than(vo, thr_out)\n",
        "\n",
        "    return [[inp_weight, rec_weight, out_weight], [thr_rec, thr_out, alpha, kappa], [v, z, vo, zo]], [z, zo]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "NKenhm_hJNQE"
      },
      "outputs": [],
      "source": [
        "#@title 4. Training \n",
        "\n",
        "def train(key, batch_size, n_inp, n_rec, n_out, thr_rec, thr_out, tau_rec, \n",
        "          lr, tau_out, w_gain, n_epochs, num_prefetch):\n",
        "    \n",
        "    key, key_model = random.split(key, 2)\n",
        "    n_devices = local_device_count()\n",
        "\n",
        "    def net_step(net_params, x_t):\n",
        "        ''' Single time step network inference (x_t => yhat_t)\n",
        "        '''\n",
        "        net_params, [z_rec, z_out] = lif_forward(net_params, x_t)\n",
        "        return net_params, [z_rec, z_out]\n",
        "    \n",
        "    def predict(weights, X):\n",
        "        _, net_const, net_dyn = param_initializer(key, n_inp, n_rec, n_out, thr_rec,\n",
        "                                                  thr_out, tau_rec, tau_out, w_gain)\n",
        "        _, [z_rec, z_out] = scan(net_step, [weights, net_const, net_dyn], X, length=100) \n",
        "        Yhat = log_softmax(jnp.sum(z_out.T, 1))\n",
        "        return Yhat, [z_rec, z_out]\n",
        "\n",
        "    v_predict = vmap(predict, in_axes=(None, 0))\n",
        "\n",
        "    def loss(weight, X, Y):\n",
        "        ''' Scan over time and return predictions\n",
        "        '''\n",
        "        Yhat, [z_rec, z_out] = v_predict(weight, X)\n",
        "        Y = one_hot(Y, n_out)\n",
        "        num_correct = jnp.sum(jnp.equal(jnp.argmax(Yhat, 1), jnp.argmax(Y, 1)))\n",
        "        loss_ce = -jnp.mean(jnp.sum(Yhat * Y, axis=1, dtype=jnp.float32))\n",
        "        return loss_ce, num_correct\n",
        "\n",
        "    def param_initializer(key, n_inp, n_rec, n_out, thr_rec, thr_out, tau_rec, tau_out, w_gain):\n",
        "        ''' Initialize parameters\n",
        "        '''\n",
        "        key_inp, key_rec, key_out, key = random.split(key, 4)\n",
        "        alpha = jnp.exp(-1e-3/tau_rec) \n",
        "        kappa = jnp.exp(-1e-3/tau_out)\n",
        "\n",
        "        inp_weight = random.normal(key_inp, [n_inp, n_rec]) * w_gain\n",
        "        rec_weight = random.normal(key_rec, [n_rec, n_rec]) * w_gain\n",
        "        out_weight = random.normal(key_out, [n_rec, n_out]) * w_gain\n",
        "\n",
        "        neuron_dyn = [jnp.zeros(n_rec), jnp.zeros(n_rec), jnp.zeros(n_out), jnp.zeros(n_out)]\n",
        "        net_params = [[inp_weight, rec_weight, out_weight], [thr_rec, thr_out, alpha, kappa], neuron_dyn]\n",
        "        return net_params\n",
        " \n",
        "    @partial(pmap, axis_name='num_devices', donate_argnums=(0))   \n",
        "    def update(opt_state, X, Y):\n",
        "        weight = get_params(opt_state)\n",
        "        value, grads = value_and_grad(loss, has_aux=True)(weight, X, Y)\n",
        "        grads = jax.lax.pmean(grads, axis_name='num_devices')\n",
        "        L = jax.lax.pmean(value[0], axis_name='num_devices')\n",
        "        tot_corr = jax.lax.psum(value[1], axis_name='num_devices')\n",
        "        opt_state = opt_update(0, grads, opt_state)\n",
        "        return opt_state, (L, tot_corr)\n",
        "\n",
        "    def one_hot(x, n_class):\n",
        "        return jnp.array(x[:, None] == jnp.arange(n_class), dtype=jnp.float32)\n",
        "\n",
        "    @partial(pmap, axis_name='num_devices')   \n",
        "    def total_correct(opt_state, X, Y):\n",
        "        weight = get_params(opt_state)\n",
        "        L, tot_corr = loss(weight, X, Y)\n",
        "        p_tot_corr = jax.lax.psum(tot_corr, axis_name='num_devices')\n",
        "        return p_tot_corr\n",
        "\n",
        "    piecewise_lr = optimizers.piecewise_constant([1000], [lr, lr/10])\n",
        "    opt_init, opt_update, get_params = optimizers.adam(step_size=piecewise_lr)\n",
        "    weight, _, _ = param_initializer(key, n_inp, n_rec, n_out, thr_rec, \\\n",
        "                                     thr_out, tau_rec, tau_out, w_gain)\n",
        "    opt_state = opt_init(weight)\n",
        "    opt_state = jax.device_put_replicated(opt_state, jax.local_devices())\n",
        "\n",
        "    # Preprocess data\n",
        "    train_shd_file, test_shd_file = get_h5py_files()\n",
        "    train_x, train_y = preprocess_h5py_files(train_shd_file)\n",
        "    test_x, test_y = preprocess_h5py_files(test_shd_file)\n",
        "    train_ds = create_tf_dataset(train_x, train_y, batch_size=batch_size, training=True)\n",
        "    test_ds = create_tf_dataset(test_x, test_y, batch_size=batch_size, training=False)\n",
        "    del train_x, train_y, test_x, test_y\n",
        "\n",
        "    # Training loop\n",
        "    train_loss = []; t = time.time(); \n",
        "    for epoch in range(n_epochs):\n",
        "        acc = 0\n",
        "        for batch_idx, b in enumerate(prefetch(train_ds, num_prefetch)):\n",
        "            opt_state, (L, tot_correct) = update(opt_state, b['spikes'], b['labels'])\n",
        "            train_loss.append(L)\n",
        "            acc += tot_correct\n",
        "        \n",
        "        # Logs\n",
        "        if epoch % 10 == 0:\n",
        "            train_acc = 100*acc/(((batch_idx)+1)*batch_size*jax.device_count())\n",
        "            print(f'Epoch: {epoch}/{n_epochs}' + \n",
        "                  f' - Loss: {jnp.mean(L):.2f}' +\n",
        "                  f' - Training acc: {jnp.mean(train_acc):.2f}')\n",
        "    \n",
        "    t_end = time.time()\n",
        "    print(f'Training completed in {(t_end-t):.2f} seconds ({(n_epochs/(t_end-t)):.2f} epoch/s)')\n",
        "\n",
        "    # Testing loop\n",
        "    test_acc_shd = 0; tot_inp = 0; tot_corr = 0\n",
        "    for batch_idx, b in enumerate(test_ds):\n",
        "        tot_inp  += b['spikes'].shape[0] * b['spikes'].shape[1]\n",
        "        tot_corr += total_correct(opt_state, b['spikes'], b['labels'])[0]\n",
        "    test_acc_shd = 100*tot_corr/tot_inp\n",
        "    print(f'SHD Test Accuracy: {test_acc_shd:.1f}%')\n",
        "\n",
        "    return train_loss, test_acc_shd, weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwgX81r0JjuF",
        "outputId": "e431954d-6d11-4446-e5cd-b5eada1a7669"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available at: /content/audiospikes/shd_train.h5\n",
            "Available at: /content/audiospikes/shd_test.h5\n",
            "Epoch: 0/250 - Loss: 17.07 - Training acc: 5.96\n",
            "Epoch: 10/250 - Loss: 2.90 - Training acc: 5.65\n",
            "Epoch: 20/250 - Loss: 2.42 - Training acc: 21.16\n",
            "Epoch: 30/250 - Loss: 1.56 - Training acc: 46.11\n",
            "Epoch: 40/250 - Loss: 1.03 - Training acc: 63.17\n",
            "Epoch: 50/250 - Loss: 0.72 - Training acc: 73.54\n",
            "Epoch: 60/250 - Loss: 0.52 - Training acc: 80.41\n",
            "Epoch: 70/250 - Loss: 0.70 - Training acc: 77.75\n",
            "Epoch: 80/250 - Loss: 0.42 - Training acc: 84.32\n",
            "Epoch: 90/250 - Loss: 0.30 - Training acc: 88.67\n",
            "Epoch: 100/250 - Loss: 0.28 - Training acc: 90.65\n",
            "Epoch: 110/250 - Loss: 0.27 - Training acc: 90.54\n",
            "Epoch: 120/250 - Loss: 0.19 - Training acc: 92.69\n",
            "Epoch: 130/250 - Loss: 0.16 - Training acc: 94.46\n",
            "Epoch: 140/250 - Loss: 0.13 - Training acc: 95.76\n",
            "Epoch: 150/250 - Loss: 0.12 - Training acc: 95.44\n",
            "Epoch: 160/250 - Loss: 0.13 - Training acc: 96.48\n",
            "Epoch: 170/250 - Loss: 0.15 - Training acc: 96.61\n",
            "Epoch: 180/250 - Loss: 0.25 - Training acc: 91.85\n",
            "Epoch: 190/250 - Loss: 0.27 - Training acc: 90.15\n",
            "Epoch: 200/250 - Loss: 0.09 - Training acc: 96.94\n",
            "Epoch: 210/250 - Loss: 0.07 - Training acc: 96.00\n",
            "Epoch: 220/250 - Loss: 0.06 - Training acc: 98.79\n",
            "Epoch: 230/250 - Loss: 0.05 - Training acc: 99.08\n",
            "Epoch: 240/250 - Loss: 0.03 - Training acc: 99.33\n",
            "Training completed in 48.46 seconds (5.16 epoch/s)\n",
            "SHD Test Accuracy: 76.3%\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters \n",
        "seed = 42\n",
        "batch_size = 1024\n",
        "n_epochs = 250\n",
        "lr = 2e-3\n",
        "n_inp = 700\n",
        "n_rec = 256\n",
        "n_out = 20\n",
        "thr_rec = 1\n",
        "thr_out = 1\n",
        "tau_rec = 20e-3\n",
        "tau_out = 20e-3\n",
        "w_gain = 1e-1\n",
        "num_prefetch = 4\n",
        "\n",
        "train_loss, test_acc_shd, weights = train(key=random.PRNGKey(seed), \n",
        "                                          batch_size=batch_size, \n",
        "                                          n_inp=n_inp,\n",
        "                                          n_rec=n_rec,\n",
        "                                          n_out=n_out,\n",
        "                                          thr_rec=thr_rec,\n",
        "                                          thr_out=thr_out,\n",
        "                                          tau_rec=tau_rec,\n",
        "                                          lr=lr,\n",
        "                                          tau_out=tau_out,\n",
        "                                          w_gain=w_gain,\n",
        "                                          n_epochs=n_epochs,\n",
        "                                          num_prefetch=num_prefetch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WyuBeGrdwENV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
